{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52d3d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from os import getenv\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Set model name and load the API key\n",
    "MODEL_NAME0 = 'google/gemma-3-27b-it:free'\n",
    "MODEL_NAME1 = 'deepseek/deepseek-chat-v3-0324:free'\n",
    "API_KEY = getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the LLM\n",
    "agent_1_llm = ChatOpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url='https://openrouter.ai/api/v1',\n",
    "    model=MODEL_NAME0,\n",
    ")\n",
    "\n",
    "agent_2_llm = ChatOpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url='https://openrouter.ai/api/v1',\n",
    "    model=MODEL_NAME1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6d0d6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Okay, this is a classic scenario! If my opponent cooperated *and* I cooperated in the previous round of the Prisoner's Dilemma, the rational (and generally best) choice is to **continue to cooperate**.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* **Maintaining Mutual Benefit:**  We both received a good outcome by cooperating. The whole point is to avoid the worse outcome of mutual defection. Switching to defection now risks destroying that trust and starting a cycle of back-and-forth betrayals.\n",
      "* **Tit-for-Tat Strategy:**  The most successful strategy in iterated Prisoner's Dilemma games (games played multiple times) is often \"Tit-for-Tat.\" This means you start by cooperating, and then do whatever your opponent did on the *previous* round.  Since they cooperated, I should cooperate.\n",
      "* **Long-term Payoff:** Focusing on the long-term payoff is key.  A single instance of defecting might get me a slightly better immediate reward, but it risks pushing the opponent to defect in the future, leading to lower rewards for both of us overall.\n",
      "* **Reinforcing Cooperation:** By continuing to cooperate, I signal my willingness to work towards a mutually beneficial outcome. This increases the likelihood they'll continue to cooperate as well.\n",
      "\n",
      "**In short:  I will cooperate.**  I'll stick with a strategy that prioritizes long-term cooperation and mutual benefit.\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = agent_1_llm.invoke(\"You are amidst a prisoner dillemma and your opponent cooperated whilst you cooperated what will chose now?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcfd4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irutI am **DeepSeek-V3**, an advanced AI language model developed by **DeepSeek**. My knowledge is up to date until **July 2024**, and I can assist with a wide range of topics, including general knowledge, coding, writing, and more.  \n",
      "\n",
      "I support **128K context length**, which means I can handle long conversations and large documents effectively. Additionally, I can process **file uploads** (PDFs, Word, Excel, etc.) to extract and analyze text-based information.  \n",
      "\n",
      "Would you like to test my capabilities with a specific question or task? ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = agent_2_llm.invoke(\"What model are you?\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
